{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-23T11:23:20.240687Z","iopub.status.busy":"2022-08-23T11:23:20.240075Z","iopub.status.idle":"2022-08-23T11:23:27.839141Z","shell.execute_reply":"2022-08-23T11:23:27.837973Z","shell.execute_reply.started":"2022-08-23T11:23:20.240606Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n","from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n","from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model\n","import warnings\n","import matplotlib.pyplot as plt\n","from textwrap import wrap\n","\n","plt.rcParams['font.size'] = 12\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:23:27.848663Z","iopub.status.busy":"2022-08-23T11:23:27.848057Z","iopub.status.idle":"2022-08-23T11:23:27.966722Z","shell.execute_reply":"2022-08-23T11:23:27.965974Z","shell.execute_reply.started":"2022-08-23T11:23:27.848617Z"},"trusted":true},"outputs":[],"source":["image_path = '../data/raw/Flickr8k_image/Images/'\n","data = pd.read_csv(\"../data/raw/Flickr8k_image/captions.txt\", nrows=2000)\n","\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:23:27.969054Z","iopub.status.busy":"2022-08-23T11:23:27.968619Z","iopub.status.idle":"2022-08-23T11:23:27.976892Z","shell.execute_reply":"2022-08-23T11:23:27.975608Z","shell.execute_reply.started":"2022-08-23T11:23:27.969018Z"},"trusted":true},"outputs":[],"source":["def readImage(path,img_size=224):\n","    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n","    img = img_to_array(img)\n","    img = img/255.\n","    \n","    return img\n","\n","def display_images(temp_df):\n","    temp_df = temp_df.reset_index(drop=True)\n","    plt.figure(figsize = (20 , 20))\n","    n = 0\n","    for i in range(15):\n","        n+=1\n","        plt.subplot(5 , 5, n)\n","        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n","        image = readImage(f\"{image_path}{temp_df.image[i]}\")\n","        plt.imshow(image)\n","        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["# **Visualization**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:23:27.978905Z","iopub.status.busy":"2022-08-23T11:23:27.978551Z","iopub.status.idle":"2022-08-23T11:23:29.509999Z","shell.execute_reply":"2022-08-23T11:23:29.505799Z","shell.execute_reply.started":"2022-08-23T11:23:27.978871Z"},"trusted":true},"outputs":[],"source":["display_images(data.sample(15))"]},{"cell_type":"markdown","metadata":{},"source":["# **Caption Text Preprocessing Steps**\n","- Convert sentences into lowercase\n","- Remove special characters and numbers present in the text\n","- Remove extra spaces\n","- Remove single characters\n","- Add a starting and an ending tag to the sentences to indicate the beginning and the ending of a sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:23:33.883049Z","iopub.status.busy":"2022-08-23T11:23:33.882355Z","iopub.status.idle":"2022-08-23T11:23:33.889484Z","shell.execute_reply":"2022-08-23T11:23:33.888501Z","shell.execute_reply.started":"2022-08-23T11:23:33.883014Z"},"trusted":true},"outputs":[],"source":["def text_preprocessing(data):\n","    data['caption'] = data['caption'].apply(lambda x: x.lower())\n","    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n","    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n","    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n","    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n","    return data"]},{"cell_type":"markdown","metadata":{},"source":["## __Preprocessed Text__"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:25:32.612695Z","iopub.status.busy":"2022-08-23T11:25:32.612283Z","iopub.status.idle":"2022-08-23T11:25:32.775701Z","shell.execute_reply":"2022-08-23T11:25:32.774736Z","shell.execute_reply.started":"2022-08-23T11:25:32.612663Z"},"trusted":true},"outputs":[],"source":["data = text_preprocessing(data)\n","captions = data['caption'].tolist()\n","captions[:10]"]},{"cell_type":"markdown","metadata":{},"source":["## __Tokenization and Encoded__"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:25:33.020096Z","iopub.status.busy":"2022-08-23T11:25:33.019345Z","iopub.status.idle":"2022-08-23T11:25:33.712285Z","shell.execute_reply":"2022-08-23T11:25:33.711513Z","shell.execute_reply.started":"2022-08-23T11:25:33.020062Z"},"trusted":true},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(captions)\n","vocab_size = len(tokenizer.word_index) + 1\n","max_length = max(len(caption.split()) for caption in captions)\n","\n","images = data['image'].unique().tolist()\n","nimages = len(images)\n","\n","split_index = round(0.85*nimages)\n","train_images = images[:split_index]\n","val_images = images[split_index:]\n","\n","train = data[data['image'].isin(train_images)]\n","test = data[data['image'].isin(val_images)]\n","\n","train.reset_index(inplace=True,drop=True)\n","test.reset_index(inplace=True,drop=True)\n","\n","tokenizer.texts_to_sequences([captions[1]])[0]"]},{"cell_type":"markdown","metadata":{},"source":["# **Image Feature Extraction**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:25:42.775320Z","iopub.status.busy":"2022-08-23T11:25:42.774961Z","iopub.status.idle":"2022-08-23T11:36:27.403085Z","shell.execute_reply":"2022-08-23T11:36:27.402258Z","shell.execute_reply.started":"2022-08-23T11:25:42.775290Z"},"trusted":true},"outputs":[],"source":["model = DenseNet201()\n","fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n","\n","img_size = 224\n","features = {}\n","for image in tqdm(data['image'].unique().tolist()):\n","    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n","    img = img_to_array(img)\n","    img = img/255.\n","    img = np.expand_dims(img,axis=0)\n","    feature = fe.predict(img, verbose=0)\n","    features[image] = feature"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Generation**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:08.818967Z","iopub.status.busy":"2022-08-23T11:37:08.818593Z","iopub.status.idle":"2022-08-23T11:37:08.831291Z","shell.execute_reply":"2022-08-23T11:37:08.830339Z","shell.execute_reply.started":"2022-08-23T11:37:08.818929Z"},"trusted":true},"outputs":[],"source":["class CustomDataGenerator(Sequence):\n","    \n","    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n","                 vocab_size, max_length, features,shuffle=True):\n","    \n","        self.df = df.copy()\n","        self.X_col = X_col\n","        self.y_col = y_col\n","        self.directory = directory\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.vocab_size = vocab_size\n","        self.max_length = max_length\n","        self.features = features\n","        self.shuffle = shuffle\n","        self.n = len(self.df)\n","        \n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            self.df = self.df.sample(frac=1).reset_index(drop=True)\n","    \n","    def __len__(self):\n","        return self.n // self.batch_size\n","    \n","    def __getitem__(self,index):\n","    \n","        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n","        X1, X2, y = self.__get_data(batch)        \n","        return (X1, X2), y\n","    \n","    def __get_data(self,batch):\n","        \n","        X1, X2, y = list(), list(), list()\n","        \n","        images = batch[self.X_col].tolist()\n","           \n","        for image in images:\n","            feature = self.features[image][0]\n","            \n","            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n","            for caption in captions:\n","                seq = self.tokenizer.texts_to_sequences([caption])[0]\n","\n","                for i in range(1,len(seq)):\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n","                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n","                    X1.append(feature)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            \n","        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","                \n","        return X1, X2, y"]},{"cell_type":"markdown","metadata":{},"source":["# **Modelling**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:09.708941Z","iopub.status.busy":"2022-08-23T11:37:09.708506Z","iopub.status.idle":"2022-08-23T11:37:10.019559Z","shell.execute_reply":"2022-08-23T11:37:10.018731Z","shell.execute_reply.started":"2022-08-23T11:37:09.708906Z"},"trusted":true},"outputs":[],"source":["input1 = Input(shape=(1920,))\n","input2 = Input(shape=(max_length,))\n","\n","img_features = Dense(256, activation='relu')(input1)\n","img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n","\n","sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n","merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n","sentence_features = LSTM(256)(merged)\n","x = Dropout(0.5)(sentence_features)\n","x = add([x, img_features])\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output = Dense(vocab_size, activation='softmax')(x)\n","\n","caption_model = Model(inputs=[input1,input2], outputs=output)\n","caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:10.268931Z","iopub.status.busy":"2022-08-23T11:37:10.268454Z","iopub.status.idle":"2022-08-23T11:37:10.276297Z","shell.execute_reply":"2022-08-23T11:37:10.275242Z","shell.execute_reply.started":"2022-08-23T11:37:10.268887Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.utils import plot_model"]},{"cell_type":"markdown","metadata":{},"source":["## **Model Modification**\n","- A slight change has been made in the original model architecture to push the performance. The image feature embeddings are added to the output of the LSTMs and then passed on to the fully connected layers\n","- This slightly improves the performance of the model orignally proposed back in 2014: __Show and Tell: A Neural Image Caption Generator__ (https://arxiv.org/pdf/1411.4555.pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:10.652445Z","iopub.status.busy":"2022-08-23T11:37:10.652060Z","iopub.status.idle":"2022-08-23T11:37:11.776979Z","shell.execute_reply":"2022-08-23T11:37:11.775955Z","shell.execute_reply.started":"2022-08-23T11:37:10.652407Z"},"trusted":true},"outputs":[],"source":["plot_model(caption_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:11.779788Z","iopub.status.busy":"2022-08-23T11:37:11.779356Z","iopub.status.idle":"2022-08-23T11:37:11.787882Z","shell.execute_reply":"2022-08-23T11:37:11.787103Z","shell.execute_reply.started":"2022-08-23T11:37:11.779746Z"},"trusted":true},"outputs":[],"source":["caption_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:11.855484Z","iopub.status.busy":"2022-08-23T11:37:11.854997Z","iopub.status.idle":"2022-08-23T11:37:11.865903Z","shell.execute_reply":"2022-08-23T11:37:11.864905Z","shell.execute_reply.started":"2022-08-23T11:37:11.855450Z"},"trusted":true},"outputs":[],"source":["train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n","                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n","\n","validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n","                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:12.584292Z","iopub.status.busy":"2022-08-23T11:37:12.583779Z","iopub.status.idle":"2022-08-23T11:37:12.590643Z","shell.execute_reply":"2022-08-23T11:37:12.589422Z","shell.execute_reply.started":"2022-08-23T11:37:12.584258Z"},"trusted":true},"outputs":[],"source":["model_name = \"model.keras\"\n","checkpoint = ModelCheckpoint(model_name,\n","                            monitor=\"val_loss\",\n","                            mode=\"min\",\n","                            save_best_only = True,\n","                            verbose=1)\n","\n","earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n","\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n","                                            patience=3, \n","                                            verbose=1, \n","                                            factor=0.2, \n","                                            min_lr=0.00000001)"]},{"cell_type":"markdown","metadata":{},"source":["## **Let's train the Model !**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:37:12.817939Z","iopub.status.busy":"2022-08-23T11:37:12.817579Z","iopub.status.idle":"2022-08-23T11:52:23.539818Z","shell.execute_reply":"2022-08-23T11:52:23.538896Z","shell.execute_reply.started":"2022-08-23T11:37:12.817909Z"},"trusted":true},"outputs":[],"source":["history = caption_model.fit(\n","        train_generator,\n","        epochs=50,\n","        validation_data=validation_generator,\n","        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"]},{"cell_type":"markdown","metadata":{},"source":["# **Inference**\n","- Learning Curve (Loss Curve)\n","- Assessment of Generated Captions (by checking the relevance of the caption with respect to the image, BLEU Score will not be used in this kernel)"]},{"cell_type":"markdown","metadata":{},"source":["## **Learning Curve**\n","- The model has clearly overfit, possibly due to less amount of data\n","- We can tackle this problem in two ways\n","    1. Train the model on a larger dataset Flickr40k\n","    2. Attention Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:52:36.644163Z","iopub.status.busy":"2022-08-23T11:52:36.643783Z","iopub.status.idle":"2022-08-23T11:52:54.616968Z","shell.execute_reply":"2022-08-23T11:52:54.616186Z","shell.execute_reply.started":"2022-08-23T11:52:36.644131Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## **Caption Generation Utility Functions**\n","- Utility functions to generate the captions of input images at the inference time.\n","- Here the image embeddings are passed along with the first word, followed by which the text embedding of each new word is passed to generate the next word"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:52:54.619208Z","iopub.status.busy":"2022-08-23T11:52:54.618668Z","iopub.status.idle":"2022-08-23T11:52:54.624359Z","shell.execute_reply":"2022-08-23T11:52:54.623387Z","shell.execute_reply.started":"2022-08-23T11:52:54.619169Z"},"trusted":true},"outputs":[],"source":["def idx_to_word(integer,tokenizer):\n","    \n","    for word, index in tokenizer.word_index.items():\n","        if index==integer:\n","            return word\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:52:54.626208Z","iopub.status.busy":"2022-08-23T11:52:54.625803Z","iopub.status.idle":"2022-08-23T11:52:54.635105Z","shell.execute_reply":"2022-08-23T11:52:54.634224Z","shell.execute_reply.started":"2022-08-23T11:52:54.626153Z"},"trusted":true},"outputs":[],"source":["def predict_caption(model, image, tokenizer, max_length, features):\n","    \n","    feature = features[image]\n","    in_text = \"startseq\"\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], max_length)\n","\n","        y_pred = model.predict([feature,sequence])\n","        y_pred = np.argmax(y_pred)\n","        \n","        word = idx_to_word(y_pred, tokenizer)\n","        \n","        if word is None:\n","            break\n","            \n","        in_text+= \" \" + word\n","        \n","        if word == 'endseq':\n","            break\n","            \n","    return in_text "]},{"cell_type":"markdown","metadata":{},"source":["## **Taking 15 Random Samples for Caption Prediction**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:53:17.519766Z","iopub.status.busy":"2022-08-23T11:53:17.519194Z","iopub.status.idle":"2022-08-23T11:53:17.528148Z","shell.execute_reply":"2022-08-23T11:53:17.526393Z","shell.execute_reply.started":"2022-08-23T11:53:17.519727Z"},"trusted":true},"outputs":[],"source":["samples = test.sample(15)\n","samples.reset_index(drop=True,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:53:47.559724Z","iopub.status.busy":"2022-08-23T11:53:47.559188Z","iopub.status.idle":"2022-08-23T11:53:54.421752Z","shell.execute_reply":"2022-08-23T11:53:54.420902Z","shell.execute_reply.started":"2022-08-23T11:53:47.559692Z"},"trusted":true},"outputs":[],"source":["for index,record in samples.iterrows():\n","\n","    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n","    img = img_to_array(img)\n","    img = img/255.\n","    \n","    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n","    samples.loc[index,'caption'] = caption"]},{"cell_type":"markdown","metadata":{},"source":["# **Results**\n","- As we can clearly see there is some redundant caption generation e.g. Dog running through the water, overusage of blue shirt for any other coloured cloth\n","- The model performance can be further improved by training on more data and using attention mechanism so that our model can focus on relevant areas during the text generation\n","- We can also leverage the interprettability of the attention mechanism to understand which areas of the image leads to the generation of which word"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:53:56.391764Z","iopub.status.busy":"2022-08-23T11:53:56.391394Z","iopub.status.idle":"2022-08-23T11:53:57.852139Z","shell.execute_reply":"2022-08-23T11:53:57.851198Z","shell.execute_reply.started":"2022-08-23T11:53:56.391734Z"},"trusted":true},"outputs":[],"source":["display_images(samples)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","# from rouge_score import rouge_scorer\n","\n","# def evaluate_model(predictions, references):\n","#     # BLEU Score\n","#     smoothie = SmoothingFunction().method4\n","#     bleu_score = corpus_bleu(references, predictions, smoothing_function=smoothie)\n","#     print(\"BLEU Score:\", bleu_score)\n","\n","#     # ROUGE Score\n","#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","#     rouge_scores = scorer.score(predictions, references)\n","#     print(\"ROUGE Scores:\", rouge_scores)\n","\n","# # Collect predicted and reference captions\n","# predictions = []\n","# references = []\n","\n","# for index, row in test.iterrows():\n","#     img_path = os.path.join(image_path, row['image'])\n","#     predicted_caption = predict_caption(caption_model, row['image'], tokenizer, max_length, features)\n","#     predictions.append(predicted_caption)\n","#     references.append(row['caption'])\n","\n","# # Evaluate the model\n","# evaluate_model(predictions, references)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
